{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos que usei para a dissertação nao tem o ruído de saída como parâmetro (são fixos durante todo o treino e determinados por validação cruzada), com excessão do **Probabilistic Backprop** (**PBP**) cuja saída é média e variância preditivas e a variância estimada para o ruído aditivo de observação.\n",
    "\n",
    "Uma abordagem mais interessante é estabelecer tal variável de ruído como parâmetro tunável interno do modelo. Como participa do cálculo da função objetivo, terá seu valor atualizado pelo otimizador. Nesse caso, haverá um parâmetro de ruído único para toda e qualquer entrada, logo o modelo será homocedástico. Note que esse representará o ruído dos dados (incerteza aleátoria) e não a do modelo (incerteza epistêmica), essa última é calculada fazendo-se MC nos pesos da rede e calculando a variância observada. Caso se deseje que o modelo seja capaz de predizer a incerteza heterocedástica, o parâmetro de variância deverá fazer parte da saída da rede. Os dois tipos de variância podem coexistir no modelo.\n",
    "\n",
    "Para que sejam compatíveis com o **PBP**, certamente deve ser implementado nos outros modelos a capacidade de prever o ruído aditivo de saída (o parâmetro tunável atrelado ao modelo). E quanto a predição da variância pela rede, deve ser inclusa também ou apenas a oriunda da integração por amostragem de MC é suficiente?\n",
    "\n",
    "Refletindo sobre o significado e origem desses dois termos, chega-se a conclusão que o MC é suficiente para os outros modelos e a rede não deve prever um termo de variância. Esse último corresponde a uma variânca que a rede preve para a *entrada* enquanto o PBP calcula sua variância analiticamente a partir da distribuição dos *pesos* e isso (variância oriunda dos pesos) é feito nos outros modelos por MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import theano\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch import autograd\n",
    "\n",
    "from bayesnn.datasets import Dataset\n",
    "\n",
    "from bayesnn.uci_code.experiments_pbp import ExperimentPBPReg\n",
    "from bayesnn.uci_code.experiments import ExperimentBBBMLPReg, ExperimentVadamMLPReg, ExperimentDropoutMLPReg\n",
    "\n",
    "from bayesnn.utils import plot_1d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set('paper', 'white', 'colorblind', font_scale=2.2,\n",
    "        rc={'lines.linewidth': 2,\n",
    "            'lines.markersize': 10,\n",
    "            'figure.figsize': (6.0, 6.0),\n",
    "            'image.interpolation': 'nearest',\n",
    "            'image.cmap': 'gray',\n",
    "            'text.usetex' : True,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(x_train, y_train, x_domain, means, aleatoric, epistemic, ideal_output=None):\n",
    "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
    "\n",
    "    fig = plt.figure(figsize = (6, 5))\n",
    "    plt.style.use('default')\n",
    "    c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
    "         '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "    plt.scatter(x_train, y_train, s = 10, marker = 'x', color = c[3], alpha = 0.5, label = 'data samples')\n",
    "\n",
    "    plt.fill_between(x_domain, means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = r'model uncertainty')\n",
    "    plt.fill_between(x_domain, means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
    "    plt.fill_between(x_domain, means - aleatoric, means + aleatoric, color = c[1], alpha = 0.4, label = r'data uncertainty')\n",
    "\n",
    "    plt.plot(x_domain, means, color = 'black', linewidth = 1, label = 'model mean')\n",
    "    \n",
    "    if ideal_output is not None:\n",
    "        plt.plot(x_domain, ideal_output, color = c[3], linewidth = 1)\n",
    "    \n",
    "    # plt.xlim([-5, 5])\n",
    "    plt.ylim([-8, 11])\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "\n",
    "    plt.gca().yaxis.grid(alpha=0.3)\n",
    "    plt.gca().xaxis.grid(alpha=0.3)\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy set #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/deterministic-variational-inference/blob/master/ToyData.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "\n",
    "np.random.seed(2)\n",
    "no_points = 400\n",
    "lengthscale = 1\n",
    "variance = 1.0\n",
    "sig_noise = 0.3\n",
    "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
    "x.sort(axis = 0)\n",
    "\n",
    "k = GPy.kern.RBF(input_dim = 1, variance = variance, lengthscale = lengthscale)\n",
    "C = k.K(x, x) + np.eye(no_points)*sig_noise**2\n",
    "\n",
    "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
    "y = (y - y.mean())\n",
    "x_train = x[75:325]\n",
    "y_train = y[75:325]\n",
    "\n",
    "# Do similar procedure to create test samples (using the whole range)\n",
    "x_test = np.random.uniform(-5, 5, 100)[:, None]\n",
    "x_test.sort(axis = 0)\n",
    "C_test = k.K(x_test, x_test) + np.eye(100)*sig_noise**2\n",
    "y_test = np.random.multivariate_normal(np.zeros((100)), C_test)[:, None]\n",
    "y_test = (y_test - y_test.mean())\n",
    "\n",
    "# X domain for visualizing learned model\n",
    "x_domain = torch.linspace(-5,5,100)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(seed)\n",
    "\n",
    "def base_model(x):\n",
    "    return -(x+1.0)*np.sin(3 * np.pi *x)\n",
    "\n",
    "def noise_model(x):\n",
    "    return 0.3*np.ones_like(x)\n",
    "#     return 0.45*(x+0.5)**2\n",
    "\n",
    "def sample_data(x):\n",
    "    return base_model(x) + np.random.normal(0, noise_model(x))\n",
    "\n",
    "train_size, test_size = 400, 100\n",
    "\n",
    "# X values: U[-0.5, 0.5]\n",
    "x_train = np.random.rand(train_size, 1) - 0.5\n",
    "y_train = sample_data(x_train)\n",
    "\n",
    "# Y values: U[-0.5, 0.5]\n",
    "x_test = np.random.rand(test_size, 1) - 0.5\n",
    "y_test = sample_data(x_test)\n",
    "\n",
    "# X domain for visualizing learned model\n",
    "x_domain = torch.linspace(-4,4,100)[..., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes By Backprop (BBB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "#               'prior_prec': lengthscale**2, # = 1\n",
    "              'prior_prec': 1, # = 1\n",
    "              'noise_prec': 1/0.3**2 * y_train.var()}\n",
    "\n",
    "train_params={'batch_size': len(x_train),\n",
    "              'num_epochs': 2000,\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 10,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train, x_test),\n",
    "             'y_points': (y_train, y_test)}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.01,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "BBB = ExperimentBBBMLPReg(dataset,\n",
    "                            model_params,\n",
    "                            train_params,\n",
    "                            optim_params,\n",
    "                            data_params,\n",
    "                            evals_per_epoch=1,\n",
    "                            normalize_x=True,\n",
    "                            normalize_y=True,\n",
    "                            use_cuda=torch.cuda.is_available(),\n",
    "                            print_freq=199\n",
    "    )\n",
    "\n",
    "BBB.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = BBB.x_means.cpu().numpy(), BBB.x_stds.cpu().numpy()\n",
    "y_mean, y_std = BBB.y_mean.cpu().numpy(), BBB.y_std.cpu().numpy()\n",
    "\n",
    "out = BBB.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = torch.exp(-0.5*BBB.log_noise.detach()).item()*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(x_train, y_train, x_domain.view(-1).cpu().numpy()*x_std + x_mean,  means,\n",
    "                 aleatoric, epistemic)\n",
    "\n",
    "fig.savefig('figures/chap4/toy1d/bbb/large-sample.pdf', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "#               'prior_prec': lengthscale**2, # = 1\n",
    "              'prior_prec': 1, # = 1\n",
    "              'noise_prec': 1/0.3**2* y_train.var()}\n",
    "\n",
    "train_params={'batch_size': len(x_train),\n",
    "              'num_epochs': 2000,\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 10,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train[:40], x_test),\n",
    "             'y_points': (y_train[:40], y_test)}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.01,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "BBB_small = ExperimentBBBMLPReg(dataset,\n",
    "                            model_params,\n",
    "                            train_params,\n",
    "                            optim_params,\n",
    "                            data_params,\n",
    "                            evals_per_epoch=1,\n",
    "                            normalize_x=True,\n",
    "                            normalize_y=True,\n",
    "                            use_cuda=torch.cuda.is_available(),\n",
    "                            print_freq=199\n",
    "    )\n",
    "\n",
    "BBB_small.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = BBB_small.x_means.cpu().numpy(), BBB_small.x_stds.cpu().numpy()\n",
    "y_mean, y_std = BBB_small.y_mean.cpu().numpy(), BBB_small.y_std.cpu().numpy()\n",
    "\n",
    "out = BBB_small.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = torch.exp(-0.5*BBB_small.log_noise.detach()).item()*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(x_train[:40], y_train[:40], x_domain.view(-1).cpu().numpy()*x_std + x_mean,  means,\n",
    "                 aleatoric, epistemic)\n",
    "\n",
    "fig.savefig('figures/chap4/toy1d/bbb/small-sample.pdf', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Dropout (MCDO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "              'dropout': 0.03 ,\n",
    "              'prior_prec': 1, # lengthscale = 1\n",
    "              'noise_prec': 1/0.3**2 * y_train.var()}\n",
    "\n",
    "train_params={'batch_size': len(x_train),\n",
    "              'num_epochs': 10000, # 20000\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 1,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.01,\n",
    "#               'betas': 0.,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train, x_test),\n",
    "             'y_points': (y_train, y_test)}\n",
    "\n",
    "# Manually set a value for weight decay (wd) by forcing the prior precision\n",
    "# wd = 1e-4\n",
    "# model_params['prior_prec'] = 10\n",
    "# model_params['prior_prec'] = (2 * model_params['noise_prec'] * len(x_train))*wd/(1 - model_params[\"dropout\"])\n",
    "\n",
    "print('prior prec', model_params['prior_prec'])\n",
    "print('lenghtscale', np.sqrt(model_params[\"prior_prec\"]))\n",
    "print('wd', (1 - model_params[\"dropout\"]) * np.sqrt(model_params[\"prior_prec\"]) ** 2 / (2 * model_params[\"noise_prec\"] * len(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dropout = ExperimentDropoutMLPReg(dataset,\n",
    "                                    model_params,\n",
    "                                    train_params,\n",
    "                                    optim_params,\n",
    "                                    data_params=data_params,\n",
    "                                    evals_per_epoch=1,\n",
    "                                    normalize_x=True,\n",
    "                                    normalize_y=True,\n",
    "                                    use_cuda=torch.cuda.is_available(),\n",
    "                                    print_freq=1999)\n",
    "\n",
    "Dropout.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = Dropout.x_means.cpu().numpy(), Dropout.x_stds.cpu().numpy()\n",
    "y_mean, y_std = Dropout.y_mean.cpu().numpy(), Dropout.y_std.cpu().numpy()\n",
    "\n",
    "out = Dropout.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(x_train, y_train, x_domain.view(-1).cpu().numpy()*x_std + x_mean,  means, aleatoric, epistemic)\n",
    "fig.savefig('figures/chap4/toy1d/mcdo/large-sample.pdf', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "              'dropout': 0.05 ,\n",
    "              'prior_prec': 1, # lengthscale = 1\n",
    "              'noise_prec': 1/0.3**2 * y_train.var()}\n",
    "\n",
    "train_params={'batch_size': len(x_train),\n",
    "              'num_epochs': 10000, # 20000\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 1,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.01,\n",
    "#               'betas': 0.,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train[:40], x_test),\n",
    "             'y_points': (y_train[:40], y_test)}\n",
    "\n",
    "# Manually set a value for weight decay (wd) by forcing the prior precision\n",
    "# wd = 1e-4\n",
    "# model_params['prior_prec'] = 10\n",
    "# model_params['prior_prec'] = (2 * model_params['noise_prec'] * len(x_train))*wd/(1 - model_params[\"dropout\"])\n",
    "\n",
    "print('prior prec', model_params['prior_prec'])\n",
    "print('lenghtscale', np.sqrt(model_params[\"prior_prec\"]))\n",
    "print('wd', (1 - model_params[\"dropout\"]) * np.sqrt(model_params[\"prior_prec\"]) ** 2 / (2 * model_params[\"noise_prec\"] * len(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout_small = ExperimentDropoutMLPReg(dataset,\n",
    "                                    model_params,\n",
    "                                    train_params,\n",
    "                                    optim_params,\n",
    "                                    data_params=data_params,\n",
    "                                    evals_per_epoch=1,\n",
    "                                    normalize_x=True,\n",
    "                                    normalize_y=True,\n",
    "                                    use_cuda=torch.cuda.is_available(),\n",
    "                                    print_freq=1999)\n",
    "\n",
    "Dropout_small.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = Dropout_small.x_means.cpu().numpy(), Dropout_small.x_stds.cpu().numpy()\n",
    "y_mean, y_std = Dropout_small.y_mean.cpu().numpy(), Dropout_small.y_std.cpu().numpy()\n",
    "\n",
    "out = Dropout_small.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(x_train[:40], y_train[:40], x_domain.view(-1).cpu().numpy()*x_std + x_mean,  means, aleatoric, epistemic)\n",
    "fig.savefig('figures/chap4/toy1d/mcdo/small-sample.pdf', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "              'dropout': 0.05 ,\n",
    "              'prior_prec': 1, # lengthscale = 1\n",
    "              'noise_prec': 1/0.3**2 * y_train.var()}\n",
    "\n",
    "train_params={'batch_size': len(x_train),\n",
    "              'num_epochs': 200, # 20000\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 1,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.01,\n",
    "#               'betas': 0.,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train, x_test),\n",
    "             'y_points': (y_train, y_test)}\n",
    "\n",
    "# Manually set a value for weight decay (wd) by forcing the prior precision\n",
    "# wd = 1e-4\n",
    "# model_params['prior_prec'] = 10\n",
    "# model_params['prior_prec'] = (2 * model_params['noise_prec'] * len(x_train))*wd/(1 - model_params[\"dropout\"])\n",
    "\n",
    "print('prior prec', model_params['prior_prec'])\n",
    "print('lenghtscale', np.sqrt(model_params[\"prior_prec\"]))\n",
    "print('wd', (1 - model_params[\"dropout\"]) * np.sqrt(model_params[\"prior_prec\"]) ** 2 / (2 * model_params[\"noise_prec\"] * len(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dropout_unconverged = ExperimentDropoutMLPReg(dataset,\n",
    "                                    model_params,\n",
    "                                    train_params,\n",
    "                                    optim_params,\n",
    "                                    data_params=data_params,\n",
    "                                    evals_per_epoch=1,\n",
    "                                    normalize_x=True,\n",
    "                                    normalize_y=True,\n",
    "                                    use_cuda=torch.cuda.is_available(),\n",
    "                                    print_freq=1999)\n",
    "\n",
    "Dropout_unconverged.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = Dropout_unconverged.x_means.cpu().numpy(), Dropout_unconverged.x_stds.cpu().numpy()\n",
    "y_mean, y_std = Dropout_unconverged.y_mean.cpu().numpy(), Dropout_unconverged.y_std.cpu().numpy()\n",
    "\n",
    "out = Dropout_unconverged.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(x_train, y_train, x_domain.view(-1).cpu().numpy()*x_std + x_mean,  means, aleatoric, epistemic)\n",
    "fig.savefig('figures/chap4/toy1d/mcdo/unconverged.pdf', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Adam (Vadam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "#               'prior_prec': lengthscale**2, # = 1\n",
    "              'prior_prec': 1, # = 1\n",
    "              'noise_prec': 1/0.3**2 * y_train.var()}\n",
    "\n",
    "train_params={'batch_size': len(x_train),\n",
    "              'num_epochs': 2000, # 20000\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 10,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.1,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train, x_test),\n",
    "             'y_points': (y_train, y_test)}\n",
    "\n",
    "Vadam = ExperimentVadamMLPReg(dataset,\n",
    "                        model_params,\n",
    "                        train_params,\n",
    "                        optim_params,\n",
    "                        data_params=data_params,\n",
    "                        evals_per_epoch=1,\n",
    "                        normalize_x=True,\n",
    "                        normalize_y=True,\n",
    "                        use_cuda=torch.cuda.is_available(),\n",
    "                        print_freq=199\n",
    "    )\n",
    "Vadam.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_mean, x_std = Vadam.x_means.cpu().numpy(), Vadam.x_stds.cpu().numpy()\n",
    "y_mean, y_std = Vadam.y_mean.cpu().numpy(), Vadam.y_std.cpu().numpy()\n",
    "\n",
    "out = Vadam.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "# aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std\n",
    "aleatoric = 1/np.sqrt(1/0.3**2 * y_train.var())*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(x_train, y_train, x_domain.view(-1).cpu().numpy()*x_std + x_mean, means, aleatoric, epistemic)\n",
    "fig.axes[0].legend(loc='lower right')\n",
    "fig.savefig('figures/chap4/toy1d/vadam/large-sample.pdf', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "#               'prior_prec': lengthscale**2, # = 1\n",
    "              'prior_prec': 1, # = 1\n",
    "              'noise_prec': 1/0.3**2 * y_train.var()}\n",
    "\n",
    "train_params={'batch_size': len(x_train),\n",
    "              'num_epochs': 2000, # 20000\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 10,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.1,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train[:40], x_test),\n",
    "             'y_points': (y_train[:40], y_test)}\n",
    "\n",
    "Vadam_small = ExperimentVadamMLPReg(dataset,\n",
    "                        model_params,\n",
    "                        train_params,\n",
    "                        optim_params,\n",
    "                        data_params=data_params,\n",
    "                        evals_per_epoch=1,\n",
    "                        normalize_x=True,\n",
    "                        normalize_y=True,\n",
    "                        use_cuda=torch.cuda.is_available(),\n",
    "                        print_freq=199\n",
    "    )\n",
    "Vadam_small.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = Vadam_small.x_means.cpu().numpy(), Vadam_small.x_stds.cpu().numpy()\n",
    "y_mean, y_std = Vadam_small.y_mean.cpu().numpy(), Vadam_small.y_std.cpu().numpy()\n",
    "\n",
    "out = Vadam_small.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())à\n",
    "# aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std\n",
    "aleatoric = 1/np.sqrt(1/0.3**2 * y_train.var())*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(x_train[:40], y_train[:40], x_domain.view(-1).cpu().numpy()*x_std + x_mean, means, aleatoric, epistemic)\n",
    "fig.savefig('figures/chap4/toy1d/vadam/small-sample.pdf', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Backpropagation (PBP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100]}\n",
    "\n",
    "train_params={'num_epochs': 10,\n",
    "              'seed': 2}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train, x_test),\n",
    "             'y_points': (y_train, y_test)}\n",
    "\n",
    "PBP = ExperimentPBPReg(dataset,\n",
    "                        model_params,\n",
    "                        train_params,\n",
    "                        data_params,\n",
    "                        experiment_prefix='toydata',\n",
    "                        normalize_x=True,\n",
    "                        normalize_y=True,\n",
    "                        use_cuda=torch.cuda.is_available(),\n",
    "                        print_freq=9\n",
    "    )\n",
    "PBP.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = PBP.x_means.cpu().numpy(), PBP.x_stds.cpu().numpy()\n",
    "y_mean, y_std = PBP.y_mean.cpu().numpy(), PBP.y_std.cpu().numpy()\n",
    "\n",
    "# Model outpus are already *unnormalized*\n",
    "m, v, v_noise = PBP.model.predict(x_domain.numpy())\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = v_noise**0.5\n",
    "epistemic = v**0.5\n",
    "\n",
    "fig = plot_curve(x_train, y_train, x_domain.view(-1).cpu().numpy()*x_std + x_mean, m, aleatoric, epistemic)\n",
    "fig.savefig('figures/chap4/toy1d/pbp/large-sample.pdf', bbox_inches='tight', dpi=200)\n",
    "# filename = 'ex1_pbp.pdf'\n",
    "# fig.savefig( 'images/toy-example'+ filename, bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleatoric, epistemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40-point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100]}\n",
    "\n",
    "train_params={'num_epochs': 40,\n",
    "              'seed': 2}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train[:40], x_test),\n",
    "             'y_points': (y_train[:40], y_test)}\n",
    "\n",
    "PBP_small = ExperimentPBPReg(dataset,\n",
    "                        model_params,\n",
    "                        train_params,\n",
    "                        data_params,\n",
    "                        experiment_prefix='toydata',\n",
    "                        normalize_x=True,\n",
    "                        normalize_y=True,\n",
    "                        use_cuda=torch.cuda.is_available(),\n",
    "                        print_freq=9\n",
    "    )\n",
    "PBP_small.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = PBP_small.x_means.cpu().numpy(), PBP_small.x_stds.cpu().numpy()\n",
    "y_mean, y_std = PBP_small.y_mean.cpu().numpy(), PBP_small.y_std.cpu().numpy()\n",
    "\n",
    "# Model outpus are already *unnormalized*\n",
    "m, v, v_noise = PBP_small.model.predict(x_domain.numpy())\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = v_noise**0.5\n",
    "epistemic = v**0.5\n",
    "\n",
    "fig = plot_curve(x_train[:40], y_train[:40], x_domain.view(-1).cpu().numpy()*x_std + x_mean, m, aleatoric, epistemic)\n",
    "fig.savefig('figures/chap4/toy1d/pbp/small-sample.pdf', bbox_inches='tight', dpi=200)\n",
    "# filename = 'ex1_pbp.pdf'\n",
    "# fig.savefig( 'images/toy-example'+ filename, bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too many passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100]}\n",
    "\n",
    "train_params={'num_epochs': 200,\n",
    "              'seed': 2}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (x_train, x_test),\n",
    "             'y_points': (y_train, y_test)}\n",
    "\n",
    "PBP_shrink = ExperimentPBPReg(dataset,\n",
    "                        model_params,\n",
    "                        train_params,\n",
    "                        data_params,\n",
    "                        experiment_prefix='toydata',\n",
    "                        normalize_x=True,\n",
    "                        normalize_y=True,\n",
    "                        use_cuda=torch.cuda.is_available(),\n",
    "                        print_freq=99\n",
    "    )\n",
    "PBP_shrink.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = PBP_shrink.x_means.cpu().numpy(), PBP_shrink.x_stds.cpu().numpy()\n",
    "y_mean, y_std = PBP_shrink.y_mean.cpu().numpy(), PBP_shrink.y_std.cpu().numpy()\n",
    "\n",
    "# Model outpus are already *unnormalized*\n",
    "m, v, v_noise = PBP_shrink.model.predict(x_domain.numpy())\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "# aleatoric = 1/np.sqrt(BBB.model.log_noise.cpu().exp().detach().item())\n",
    "aleatoric = v_noise**0.5\n",
    "epistemic = v**0.5\n",
    "\n",
    "fig = plot_curve(x_train, y_train, x_domain.view(-1).cpu().numpy()*x_std + x_mean, m, aleatoric, epistemic)\n",
    "fig.savefig('figures/chap4/toy1d/pbp/shrinked-variance.pdf', bbox_inches='tight', dpi=200)\n",
    "# filename = 'ex1_pbp.pdf'\n",
    "# fig.savefig( 'images/toy-example'+ filename, bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleatoric, epistemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy set #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ThirstyScholar/bayes-by-backprop/blob/master/BBB/bnn_regression.py  \n",
    "https://arxiv.org/pdf/1502.05336.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(x):\n",
    "    return x**3\n",
    "\n",
    "def noise_model(x):\n",
    "    return 3*np.ones_like(x)\n",
    "\n",
    "def sample_data(x):\n",
    "    return base_model(x) + np.random.normal(0, noise_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, test_size = 20, 100\n",
    "\n",
    "# X values: U[-4, 4]\n",
    "train_data_2 = np.random.uniform(-4, 4, size=train_size).reshape((-1, 1))\n",
    "test_data_2 = np.random.uniform(-4, 4, size=test_size).reshape((-1, 1))\n",
    "\n",
    "train_labels_2 = sample_data(train_data_2)\n",
    "test_labels_2 = sample_data(test_data_2)\n",
    "\n",
    "# # X domain for visualizing learned model\n",
    "x_domain = torch.linspace(-6,6,100)[..., None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "              'prior_prec': 1.0,\n",
    "              'noise_prec': 1/9 * train_labels_2.var()}\n",
    "\n",
    "train_params={'batch_size': len(train_data_2),\n",
    "              'num_epochs': 2000, # 20000\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 10,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.1,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (train_data_2, test_data_2),\n",
    "             'y_points': (train_labels_2, test_labels_2)}\n",
    "\n",
    "BBB = ExperimentBBBMLPReg(dataset,\n",
    "                            model_params,\n",
    "                            train_params,\n",
    "                            optim_params,\n",
    "                            data_params=data_params,\n",
    "                            evals_per_epoch=1,\n",
    "                            normalize_x=True,\n",
    "                            normalize_y=True,\n",
    "                            use_cuda=torch.cuda.is_available(),\n",
    "                            print_freq=199\n",
    "    )\n",
    "BBB.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = BBB.x_means.cpu().numpy(), BBB.x_stds.cpu().numpy()\n",
    "y_mean, y_std = BBB.y_mean.cpu().numpy(), BBB.y_std.cpu().numpy()\n",
    "\n",
    "out = BBB.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std.numpy() + y_mean.numpy()\n",
    "\n",
    "aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std.numpy()\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "fig = plot_curve(train_data_2, train_labels_2,\n",
    "                 x_domain.view(-1)*x_std + x_mean, m,\n",
    "                 aleatoric, epistemic,\n",
    "                 ideal_output=base_model(x_domain.view(-1).cpu()*x_std+x_mean))\n",
    "\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "# filename = 'ex2_bbb.pdf'\n",
    "# fig.savefig( 'images/toy-example'+ filename, bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100],\n",
    "              'act_func': 'relu' ,\n",
    "              'dropout': 0.2,\n",
    "              'prior_prec': 1.0,\n",
    "              'noise_prec': 1/9 * train_labels_2.var()}\n",
    "\n",
    "train_params={'batch_size': len(train_data_2),\n",
    "              'num_epochs': 10000, # 20000\n",
    "              'seed': 2,\n",
    "              'train_mc_samples': 1,\n",
    "              'eval_mc_samples': 100}\n",
    "\n",
    "optim_params={'optim': 'adam',\n",
    "              'learning_rate': 0.1,\n",
    "              'betas': (0.9,0.99),\n",
    "              'prec_init': 10.0}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (train_data_2, test_data_2),\n",
    "             'y_points': (train_labels_2, test_labels_2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout = ExperimentDropoutMLPReg(dataset,\n",
    "                                    model_params,\n",
    "                                    train_params,\n",
    "                                    optim_params,\n",
    "                                    data_params=data_params,\n",
    "                                    evals_per_epoch=1,\n",
    "                                    normalize_x=True,\n",
    "                                    normalize_y=True,\n",
    "                                    use_cuda=torch.cuda.is_available(),\n",
    "                                    print_freq=2000)\n",
    "\n",
    "Dropout.run(log_metric_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = Dropout.x_means.cpu().numpy(), Dropout.x_stds.cpu().numpy()\n",
    "y_mean, y_std = Dropout.y_mean.cpu().numpy(), Dropout.y_std.cpu().numpy()\n",
    "\n",
    "out = Dropout.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std + y_mean\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "\n",
    "fig = plot_curve(train_data_2, train_labels_2,\n",
    "                 x_domain.view(-1).numpy()*x_std + x_mean, m,\n",
    "                 aleatoric, epistemic,\n",
    "                 ideal_output=base_model(x_domain.view(-1).cpu().numpy()*x_std+x_mean))\n",
    "\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "# filename = 'ex2_mcdropout.pdf'\n",
    "# fig.savefig( 'images/toy-example'+ filename, bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vadam = ExperimentVadamMLPReg(dataset,\n",
    "                        model_params,\n",
    "                        train_params,\n",
    "                        optim_params,\n",
    "                        data_params=data_params,\n",
    "                        evals_per_epoch=1,\n",
    "                        normalize_x=False,\n",
    "                        normalize_y=False,\n",
    "                        use_cuda=torch.cuda.is_available(),\n",
    "                        print_freq=1000\n",
    "    )\n",
    "Vadam.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = Vadam.x_means.cpu().numpy(), Vadam.x_stds.cpu().numpy()\n",
    "y_mean, y_std = Vadam.y_mean.cpu().numpy(), Vadam.y_std.cpu().numpy()\n",
    "\n",
    "out = Vadam.prediction(x_domain, train=False)\n",
    "out = torch.stack(out).cpu().detach().numpy()\n",
    "out = out*y_std.numpy() + y_mean.numpy()\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "aleatoric = 1/np.sqrt(model_params['noise_prec'])*y_std.numpy()\n",
    "epistemic = out.var(axis = 0)**0.5\n",
    "\n",
    "\n",
    "fig = plot_curve(train_data_2, train_labels_2,\n",
    "                 x_domain.view(-1)*x_std + x_mean, m,\n",
    "                 aleatoric, epistemic,\n",
    "                 ideal_output=base_model(x_domain.view(-1).cpu()*x_std+x_mean))\n",
    "\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "# filename = 'ex2_vadam.pdf'\n",
    "# fig.savefig( 'images/toy-example'+ filename, bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={'hidden_sizes': [100]}\n",
    "\n",
    "train_params={'batch_size': len(train_data_2),\n",
    "              'num_epochs': 40,\n",
    "              'seed': 2}\n",
    "\n",
    "dataset = 'toydata1d'\n",
    "data_params={'x_points': (train_data_2, test_data_2),\n",
    "             'y_points': (train_labels_2, test_labels_2)}\n",
    "\n",
    "PBP = ExperimentPBPReg(dataset,\n",
    "                        model_params,\n",
    "                        train_params,\n",
    "                        data_params,\n",
    "                        experiment_prefix='toydata',\n",
    "                        normalize_x=True,\n",
    "                        normalize_y=True,\n",
    "                        use_cuda=torch.cuda.is_available(),\n",
    "                        print_freq=39\n",
    "    )\n",
    "PBP.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean, x_std = PBP.x_means.cpu().numpy(), PBP.x_stds.cpu().numpy()\n",
    "y_mean, y_std = PBP.y_mean.cpu().numpy(), PBP.y_std.cpu().numpy()\n",
    "\n",
    "# Model outpus are already *unnormalized*\n",
    "m, v, v_noise = PBP.model.predict(x_domain.numpy())\n",
    "\n",
    "means = out.mean(axis = 0)\n",
    "\n",
    "aleatoric = v_noise**0.5\n",
    "epistemic = v**0.5\n",
    "\n",
    "fig = plot_curve(train_data_2, train_labels_2,\n",
    "                 x_domain.view(-1).cpu().numpy()*x_std + x_mean, m,\n",
    "                 aleatoric, epistemic,\n",
    "                 ideal_output=base_model(x_domain.view(-1).cpu().numpy()*x_std+x_mean))\n",
    "\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-100, 100])\n",
    "\n",
    "# filename = 'ex2_pbp.pdf'\n",
    "# fig.savefig( 'images/toy-example'+ filename, bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy set #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1505.05424.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$y=x+ 0.3 sin(2π(x+\\epsilon)) + 0.3 sin(4π(x+\\epsilon)) + \\epsilon,  where \\epsilon ∼ N(0,0.02) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
